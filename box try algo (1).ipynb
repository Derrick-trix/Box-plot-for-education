{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing, linear_model\n",
    "import sklearn.feature_extraction.text as txt\n",
    "\n",
    "\n",
    "train = pd.read_csv('TrainingData.csv')\n",
    "test = pd.read_csv('TestData.csv')\n",
    "sample = pd.read_csv('SubmissionFormat.csv')\n",
    "\n",
    "training_data = train[['FTE','Facility_or_Department', 'Function_Description','Fund_Description',\n",
    "                       'Job_Title_Description', 'Location_Description','Object_Description',\n",
    "                       'Position_Extra', 'Program_Description', 'SubFund_Description',\n",
    "                       'Sub_Object_Description', 'Text_1', 'Text_2','Text_3','Text_4', 'Total']]\n",
    "\n",
    "cols = ['Function','Object_Type','Operating_Status','Position_Type','Pre_K', 'Reporting',\n",
    "                'Sharing','Student_Type', 'Use']\n",
    "\n",
    "labels = train[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derri\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "it=['Facility_or_Department', 'Function_Description','Fund_Description',\n",
    "                       'Job_Title_Description', 'Location_Description','Object_Description',\n",
    "                       'Position_Extra', 'Program_Description', 'SubFund_Description',\n",
    "                       'Sub_Object_Description', 'Text_1', 'Text_2','Text_3','Text_4']\n",
    "test_data = test[['FTE','Facility_or_Department', 'Function_Description','Fund_Description',\n",
    "                       'Job_Title_Description', 'Location_Description','Object_Description',\n",
    "                       'Position_Extra', 'Program_Description', 'SubFund_Description',\n",
    "                       'Sub_Object_Description', 'Text_1', 'Text_2','Text_3','Text_4', 'Total']]\n",
    "\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for i in cols:\n",
    "    labels[i] = label_encoder.fit_transform(np.array(labels[i]))\n",
    "    \n",
    "    \n",
    "training_data = training_data.drop('FTE', axis = 1)\n",
    "training_data = training_data.drop('Total', axis = 1)\n",
    "\n",
    "test_data = test_data.drop('FTE', axis = 1)\n",
    "test_data = test_data.drop('Total', axis = 1)\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    try:\n",
    "        return \" \".join(re.findall(r'\\w+', s)).lower()\n",
    "    except:\n",
    "        return \" \".join(re.findall(r'\\w+', \"no_text\")).lower()\n",
    "\n",
    "for i in it:\n",
    "        training_data[i] = training_data[i].apply(clean)\n",
    "for i in it:\n",
    "        test_data[i] = test_data[i].apply(clean)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"combined\"] = [' '.join(row) for row in training_data[training_data.columns].values]\n",
    "test_data[\"combined\"] = [' '.join(row) for row in test_data[test_data.columns].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = txt.TfidfVectorizer(ngram_range=(1,2), max_df=1.0, min_df=10)\n",
    "hsv = txt.HashingVectorizer()\n",
    "\n",
    "\n",
    "tfidf.fit(training_data['combined'])\n",
    "hsv.fit(test_data['combined'])\n",
    "\n",
    "X_tfidf = tfidf.transform(training_data['combined'])\n",
    "X_test_tfidf = tfidf.transform(test_data['combined'])\n",
    "\n",
    "X_hsv = hsv.transform(training_data['combined'])\n",
    "X_test_hsv = hsv.transform(test_data['combined'])\n",
    "\n",
    "X = sparse.hstack((X_hsv, X_tfidf))\n",
    "X_test = sparse.hstack((X_test_hsv, X_test_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  0\n",
      "label =  1\n",
      "50064 50064\n",
      "label =  2\n",
      "50064 50064\n",
      "label =  3\n",
      "50064 50064\n",
      "label =  4\n",
      "50064 50064\n",
      "label =  5\n",
      "50064 50064\n",
      "label =  6\n",
      "50064 50064\n",
      "label =  7\n",
      "50064 50064\n",
      "label =  8\n",
      "50064 50064\n"
     ]
    }
   ],
   "source": [
    "prediction1 = []\n",
    "prediction2 = []\n",
    "prediction3 = []\n",
    "prediction4 = []\n",
    "for i in range(len(cols)):\n",
    "    print (\"label = \", i)\n",
    "    sgd1 = linear_model.SGDClassifier(loss = 'log', max_iter = 120, alpha = 0.000001)\n",
    "    sgd2 = linear_model.SGDClassifier(loss = 'log', max_iter = 120, penalty = 'l1')\n",
    "    sgd3 = linear_model.SGDClassifier(loss = 'log', max_iter = 120, penalty = 'l2')\n",
    "    sgd4 = linear_model.SGDClassifier(loss = 'log', max_iter = 120, penalty = 'elasticnet')\n",
    "    sgd1.fit(X, labels.iloc[:,i])\n",
    "    sgd2.fit(X, labels.iloc[:,i])\n",
    "    sgd3.fit(X, labels.iloc[:,i])\n",
    "    sgd4.fit(X, labels.iloc[:,i])\n",
    "    if i == 0:\n",
    "        prediction1 = sgd1.predict_proba(X_test)\n",
    "        prediction2 = sgd2.predict_proba(X_test)\n",
    "        prediction3 = sgd3.predict_proba(X_test)\n",
    "        prediction4 = sgd4.predict_proba(X_test)\n",
    "    else:\n",
    "        print(len(prediction1),len(sgd1.predict_proba(X_test)))\n",
    "        prediction1 = np.hstack((prediction1,sgd1.predict_proba(X_test)))\n",
    "        prediction2 = np.hstack((prediction2,sgd2.predict_proba(X_test)))\n",
    "        prediction3 = np.hstack((prediction3,sgd3.predict_proba(X_test)))\n",
    "        prediction4 = np.hstack((prediction4,sgd4.predict_proba(X_test)))\n",
    "        \n",
    "preds = (prediction1 + prediction2 + prediction3 + prediction4)/4.0\n",
    "\n",
    "for i in range(1, len(sample.columns)):\n",
    "    colname = sample.columns[i]\n",
    "    sample[str(colname)] = preds[:,i-1]\n",
    "\n",
    "sample.to_csv('Logy_Model.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
